{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05720b1e",
   "metadata": {},
   "source": [
    "# Materials Science 465: Computational Electron Microscopy\n",
    "## Assignment 1: Environment Setup and GitHub Integration\n",
    "\n",
    "**Due: Friday, January 9, 2026 at 11:59 PM**\n",
    "\n",
    "### Objectives\n",
    "This assignment establishes the foundational computational environment for the entire course. You will:\n",
    "\n",
    "1. Set up a complete Python environment optimized for electron microscopy analysis\n",
    "2. Configure version control workflows using Git and GitHub\n",
    "3. Demonstrate proficiency with Jupyter Lab and reproducible research practices\n",
    "4. Load and visualize sample electron microscopy data\n",
    "5. Create your first documented workflow for computational EM analysis\n",
    "\n",
    "### Evaluation Criteria\n",
    "- **Environment Setup (25%)**: Successful installation and configuration of all required packages\n",
    "- **GitHub Integration (25%)**: Proper repository setup with meaningful commit history\n",
    "- **Code Quality (25%)**: Clean, well-documented code following Python best practices\n",
    "- **Data Analysis (25%)**: Successful EM data loading, processing, and visualization\n",
    "\n",
    "### Submission Requirements\n",
    "1. Complete this Jupyter notebook with all cells executed\n",
    "2. Push to your personal GitHub repository with at least 5 meaningful commits\n",
    "3. Include a README.md file describing your setup process\n",
    "4. Submit the GitHub repository URL via Canvas\n",
    "\n",
    "**Note**: This notebook serves as both an assignment and a template for future computational EM workflows. Take time to understand each step thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c616e0a",
   "metadata": {},
   "source": [
    "## Part 1: Environment Verification and Setup\n",
    "\n",
    "Before proceeding with any computational electron microscopy work, we must verify that our Python environment is properly configured with all necessary packages. This section will test the installation of core scientific computing libraries and EM-specific tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Environment Verification Script for MS 465: Computational Electron Microscopy\n",
    "This cell verifies that all required packages are installed and accessible.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"MS 465 Environment Verification Report\")\n",
    "print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# System Information\n",
    "print(f\"\\nSystem Information:\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.architecture()[0]}\")\n",
    "\n",
    "# Test core scientific computing packages\n",
    "packages_core = {\n",
    "    'numpy': 'np',\n",
    "    'scipy': 'scipy',\n",
    "    'matplotlib': 'plt',\n",
    "    'pandas': 'pd',\n",
    "    'scikit-learn': 'sklearn',\n",
    "    'scikit-image': 'skimage',\n",
    "    'h5py': 'h5py',\n",
    "    'zarr': 'zarr'\n",
    "}\n",
    "\n",
    "print(f\"\\n1. Core Scientific Computing Packages:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for package, alias in packages_core.items():\n",
    "    try:\n",
    "        if alias == 'plt':\n",
    "            import matplotlib.pyplot as plt\n",
    "            print(f\"âœ“ {package}: {plt.matplotlib.__version__}\")\n",
    "        elif alias == 'sklearn':\n",
    "            import sklearn\n",
    "            print(f\"âœ“ {package}: {sklearn.__version__}\")\n",
    "        elif alias == 'skimage':\n",
    "            import skimage\n",
    "            print(f\"âœ“ {package}: {skimage.__version__}\")\n",
    "        else:\n",
    "            module = __import__(package)\n",
    "            version = getattr(module, '__version__', 'Unknown')\n",
    "            print(f\"âœ“ {package}: {version}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âœ— {package}: NOT INSTALLED ({e})\")\n",
    "\n",
    "# Test Jupyter and development tools\n",
    "jupyter_packages = {\n",
    "    'jupyterlab': 'jupyterlab',\n",
    "    'ipywidgets': 'ipywidgets',\n",
    "    'tqdm': 'tqdm'\n",
    "}\n",
    "\n",
    "print(f\"\\n2. Jupyter and Development Tools:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for package, module_name in jupyter_packages.items():\n",
    "    try:\n",
    "        module = __import__(module_name)\n",
    "        version = getattr(module, '__version__', 'Unknown')\n",
    "        print(f\"âœ“ {package}: {version}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âœ— {package}: NOT INSTALLED ({e})\")\n",
    "\n",
    "print(f\"\\n3. Machine Learning and Deep Learning:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test ML/DL packages\n",
    "ml_packages = ['torch', 'tensorflow']\n",
    "\n",
    "for package in ml_packages:\n",
    "    try:\n",
    "        if package == 'torch':\n",
    "            import torch\n",
    "            print(f\"âœ“ PyTorch: {torch.__version__}\")\n",
    "            if torch.cuda.is_available():\n",
    "                print(f\"  - CUDA available: {torch.cuda.get_device_name()}\")\n",
    "            else:\n",
    "                print(f\"  - CUDA available: No\")\n",
    "        elif package == 'tensorflow':\n",
    "            import tensorflow as tf\n",
    "            print(f\"âœ“ TensorFlow: {tf.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âœ— {package}: NOT INSTALLED ({e})\")\n",
    "\n",
    "print(f\"\\n4. Environment Setup Status:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check if we're in the correct conda environment\n",
    "conda_env = sys.prefix.split('/')[-1] if 'conda' in sys.prefix or 'miniconda' in sys.prefix else 'Not using conda'\n",
    "print(f\"Active Environment: {conda_env}\")\n",
    "\n",
    "if conda_env == 'ms465-2026':\n",
    "    print(\"âœ“ Correct conda environment activated\")\n",
    "else:\n",
    "    print(\"âš  Warning: Expected 'ms465-2026' environment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160d2039",
   "metadata": {},
   "source": [
    "### Testing EM-Specific Packages\n",
    "\n",
    "Now we'll verify the installation of specialized packages for electron microscopy data analysis. These packages are essential for the course and may require additional setup steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test EM-specific packages that are critical for the course\n",
    "\"\"\"\n",
    "\n",
    "print(\"5. Electron Microscopy Specific Packages:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Core EM analysis packages\n",
    "em_packages = {\n",
    "    'py4DSTEM': 'py4DSTEM',\n",
    "    'hyperspy': 'hyperspy', \n",
    "    'ase': 'ase',\n",
    "    'ncempy': 'ncempy'\n",
    "}\n",
    "\n",
    "em_status = {}\n",
    "\n",
    "for package_name, import_name in em_packages.items():\n",
    "    try:\n",
    "        module = __import__(import_name)\n",
    "        version = getattr(module, '__version__', 'Unknown')\n",
    "        print(f\"âœ“ {package_name}: {version}\")\n",
    "        em_status[package_name] = True\n",
    "    except ImportError as e:\n",
    "        print(f\"âœ— {package_name}: NOT INSTALLED\")\n",
    "        print(f\"  Installation command: pip install {package_name.lower()}\")\n",
    "        em_status[package_name] = False\n",
    "\n",
    "# Advanced EM packages (may not be required for Week 1)\n",
    "print(f\"\\n6. Advanced EM Packages (Optional for Week 1):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "advanced_em = {\n",
    "    'atomai': 'atomai',\n",
    "    'libertem': 'libertem', \n",
    "    'pyxem': 'pyxem',\n",
    "    'abtem': 'abtem'\n",
    "}\n",
    "\n",
    "for package_name, import_name in advanced_em.items():\n",
    "    try:\n",
    "        module = __import__(import_name)\n",
    "        version = getattr(module, '__version__', 'Unknown')\n",
    "        print(f\"âœ“ {package_name}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"â—‹ {package_name}: Not installed (will be needed in later weeks)\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n7. Installation Summary:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "required_count = len([p for p in em_status.values() if p])\n",
    "total_required = len(em_status)\n",
    "\n",
    "if required_count == total_required:\n",
    "    print(f\"âœ“ All required EM packages installed ({required_count}/{total_required})\")\n",
    "    print(\"  Ready to proceed with electron microscopy data analysis!\")\n",
    "else:\n",
    "    print(f\"âš  Missing packages: {total_required - required_count}/{total_required}\")\n",
    "    print(\"  Install missing packages before proceeding with EM analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69901cbc",
   "metadata": {},
   "source": [
    "## Part 2: GitHub Repository Setup and Version Control\n",
    "\n",
    "Version control is essential for reproducible computational research. In this section, you'll set up your course repository and demonstrate proper Git workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Git Configuration and Repository Status Check\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Git Configuration and Repository Status\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def run_git_command(command):\n",
    "    \"\"\"Run a git command and return the output\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(command.split(), \n",
    "                              capture_output=True, \n",
    "                              text=True, \n",
    "                              check=True)\n",
    "        return result.stdout.strip()\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"Error: {e.stderr.strip()}\"\n",
    "    except FileNotFoundError:\n",
    "        return \"Error: Git not found. Please install Git.\"\n",
    "\n",
    "# Check Git installation and version\n",
    "git_version = run_git_command(\"git --version\")\n",
    "print(f\"Git Version: {git_version}\")\n",
    "\n",
    "# Check if we're in a Git repository\n",
    "git_status = run_git_command(\"git status --porcelain\")\n",
    "is_git_repo = \"Error:\" not in git_status\n",
    "\n",
    "if is_git_repo:\n",
    "    print(\"âœ“ Currently in a Git repository\")\n",
    "    \n",
    "    # Get repository information\n",
    "    repo_url = run_git_command(\"git config --get remote.origin.url\")\n",
    "    current_branch = run_git_command(\"git branch --show-current\")\n",
    "    \n",
    "    print(f\"Repository URL: {repo_url}\")\n",
    "    print(f\"Current Branch: {current_branch}\")\n",
    "    \n",
    "    # Check for uncommitted changes\n",
    "    if git_status:\n",
    "        print(f\"âš  Uncommitted changes detected:\")\n",
    "        for line in git_status.split('\\n'):\n",
    "            print(f\"  {line}\")\n",
    "    else:\n",
    "        print(\"âœ“ Working directory clean\")\n",
    "        \n",
    "    # Show recent commits\n",
    "    print(f\"\\nRecent Commits:\")\n",
    "    recent_commits = run_git_command(\"git log --oneline -5\")\n",
    "    for line in recent_commits.split('\\n'):\n",
    "        if line:\n",
    "            print(f\"  {line}\")\n",
    "            \n",
    "else:\n",
    "    print(\"âš  Not in a Git repository\")\n",
    "    print(\"You need to initialize a Git repository for this assignment\")\n",
    "\n",
    "# Check Git configuration\n",
    "print(f\"\\nGit Configuration:\")\n",
    "git_user = run_git_command(\"git config --get user.name\")\n",
    "git_email = run_git_command(\"git config --get user.email\")\n",
    "\n",
    "print(f\"User Name: {git_user}\")\n",
    "print(f\"User Email: {git_email}\")\n",
    "\n",
    "if \"Error:\" in git_user or \"Error:\" in git_email:\n",
    "    print(\"âš  Git user information not configured\")\n",
    "    print(\"Run the following commands to configure Git:\")\n",
    "    print('  git config --global user.name \"Your Name\"')\n",
    "    print('  git config --global user.email \"your.email@example.com\"')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b19aee",
   "metadata": {},
   "source": [
    "### Action Items for Git Setup\n",
    "\n",
    "**If you haven't already, complete these steps:**\n",
    "\n",
    "1. **Create a GitHub account** at https://github.com if you don't have one\n",
    "2. **Create a new repository** named `ms465-computational-em-2026`\n",
    "3. **Clone the repository** to your local machine\n",
    "4. **Configure Git** with your name and email (see output above)\n",
    "5. **Create the initial commit** with this notebook\n",
    "\n",
    "**Repository Structure to Create:**\n",
    "```\n",
    "ms465-computational-em-2026/\n",
    "â”œâ”€â”€ README.md\n",
    "â”œâ”€â”€ week_01/\n",
    "â”‚   â”œâ”€â”€ assignment_01_environment_setup.ipynb (this file)\n",
    "â”‚   â””â”€â”€ data/\n",
    "â”œâ”€â”€ week_02/\n",
    "â”œâ”€â”€ ...\n",
    "â”œâ”€â”€ final_project/\n",
    "â””â”€â”€ .gitignore\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39978cc0",
   "metadata": {},
   "source": [
    "## Part 3: Data Management and HDF5 Handling\n",
    "\n",
    "Electron microscopy generates large datasets that require efficient storage and management. We'll demonstrate best practices using HDF5 format and proper metadata handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Demonstrate HDF5 data management for electron microscopy datasets\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"HDF5 Data Management Demonstration\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create sample EM-like data\n",
    "print(\"Creating sample electron microscopy dataset...\")\n",
    "\n",
    "# Simulate a 4D-STEM dataset: (scan_x, scan_y, detector_x, detector_y)\n",
    "scan_size = (64, 64)  # 64x64 scan positions\n",
    "detector_size = (128, 128)  # 128x128 detector pixels\n",
    "\n",
    "# Create synthetic 4D data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "sample_4d_data = np.random.poisson(10, size=(*scan_size, *detector_size)).astype(np.uint16)\n",
    "\n",
    "# Add some realistic structure (central beam and diffraction spots)\n",
    "for i in range(scan_size[0]):\n",
    "    for j in range(scan_size[1]):\n",
    "        # Central beam\n",
    "        center_x, center_y = detector_size[0]//2, detector_size[1]//2\n",
    "        y, x = np.ogrid[:detector_size[0], :detector_size[1]]\n",
    "        mask = (x - center_x)**2 + (y - center_y)**2 < 100\n",
    "        sample_4d_data[i, j][mask] += np.random.poisson(50, size=mask.sum())\n",
    "        \n",
    "        # Add some diffraction spots\n",
    "        if i > 20 and j > 20:\n",
    "            spot_x, spot_y = center_x + 30, center_y + 20\n",
    "            spot_mask = (x - spot_x)**2 + (y - spot_y)**2 < 25\n",
    "            sample_4d_data[i, j][spot_mask] += np.random.poisson(20, size=spot_mask.sum())\n",
    "\n",
    "print(f\"Generated 4D dataset with shape: {sample_4d_data.shape}\")\n",
    "print(f\"Data type: {sample_4d_data.dtype}\")\n",
    "print(f\"Memory usage: {sample_4d_data.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "# Create HDF5 file with proper structure and metadata\n",
    "hdf5_filename = \"sample_4dstem_data.h5\"\n",
    "\n",
    "print(f\"\\nCreating HDF5 file: {hdf5_filename}\")\n",
    "\n",
    "with h5py.File(hdf5_filename, \"w\") as f:\n",
    "    # Create main data group\n",
    "    data_group = f.create_group(\"4dstem_data\")\n",
    "    \n",
    "    # Store the 4D dataset with compression\n",
    "    dataset = data_group.create_dataset(\n",
    "        \"datacube\", \n",
    "        data=sample_4d_data,\n",
    "        compression=\"gzip\",\n",
    "        compression_opts=6,\n",
    "        chunks=True,  # Enable chunking for better I/O performance\n",
    "        shuffle=True  # Improve compression\n",
    "    )\n",
    "    \n",
    "    # Add comprehensive metadata\n",
    "    dataset.attrs[\"description\"] = \"Simulated 4D-STEM dataset for MS 465 course\"\n",
    "    dataset.attrs[\"created\"] = datetime.now().isoformat()\n",
    "    dataset.attrs[\"scan_shape\"] = scan_size\n",
    "    dataset.attrs[\"detector_shape\"] = detector_size\n",
    "    dataset.attrs[\"data_type\"] = str(sample_4d_data.dtype)\n",
    "    dataset.attrs[\"units\"] = \"counts\"\n",
    "    \n",
    "    # Experimental parameters (typical for 4D-STEM)\n",
    "    params_group = f.create_group(\"experimental_parameters\")\n",
    "    params_group.attrs[\"accelerating_voltage_kV\"] = 200.0\n",
    "    params_group.attrs[\"camera_length_mm\"] = 195.0\n",
    "    params_group.attrs[\"convergence_angle_mrad\"] = 1.0\n",
    "    params_group.attrs[\"pixel_size_nm\"] = 0.5\n",
    "    params_group.attrs[\"dwell_time_ms\"] = 1.0\n",
    "    params_group.attrs[\"microscope\"] = \"Simulated TEM\"\n",
    "    \n",
    "    # Processing parameters\n",
    "    processing_group = f.create_group(\"processing\")\n",
    "    processing_group.attrs[\"software\"] = \"MS 465 Assignment 1\"\n",
    "    processing_group.attrs[\"version\"] = \"1.0\"\n",
    "    processing_group.attrs[\"processed_date\"] = datetime.now().isoformat()\n",
    "    \n",
    "    print(\"âœ“ 4D dataset stored with compression\")\n",
    "    print(\"âœ“ Comprehensive metadata added\")\n",
    "    print(\"âœ“ Experimental parameters recorded\")\n",
    "\n",
    "# Verify file size and compression ratio\n",
    "original_size = sample_4d_data.nbytes\n",
    "compressed_size = os.path.getsize(hdf5_filename)\n",
    "compression_ratio = original_size / compressed_size\n",
    "\n",
    "print(f\"\\nCompression Statistics:\")\n",
    "print(f\"Original size: {original_size / 1024**2:.2f} MB\")\n",
    "print(f\"Compressed size: {compressed_size / 1024**2:.2f} MB\")\n",
    "print(f\"Compression ratio: {compression_ratio:.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read and explore the HDF5 file structure\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Reading and Exploring HDF5 Data Structure\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def explore_hdf5_structure(filename, max_depth=3, current_depth=0):\n",
    "    \"\"\"Recursively explore HDF5 file structure\"\"\"\n",
    "    indent = \"  \" * current_depth\n",
    "    \n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        if current_depth == 0:\n",
    "            print(f\"HDF5 File: {filename}\")\n",
    "            print(f\"File size: {os.path.getsize(filename) / 1024**2:.2f} MB\")\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        def print_structure(name, obj):\n",
    "            item_indent = \"  \" * (current_depth + 1)\n",
    "            if isinstance(obj, h5py.Dataset):\n",
    "                print(f\"{item_indent}ðŸ“Š {name}: {obj.shape} {obj.dtype}\")\n",
    "                if hasattr(obj, 'attrs') and len(obj.attrs) > 0:\n",
    "                    for attr_name, attr_value in obj.attrs.items():\n",
    "                        print(f\"{item_indent}   â””â”€ {attr_name}: {attr_value}\")\n",
    "            elif isinstance(obj, h5py.Group):\n",
    "                print(f\"{item_indent}ðŸ“ {name}/\")\n",
    "                if hasattr(obj, 'attrs') and len(obj.attrs) > 0:\n",
    "                    for attr_name, attr_value in obj.attrs.items():\n",
    "                        print(f\"{item_indent}   â””â”€ {attr_name}: {attr_value}\")\n",
    "        \n",
    "        f.visititems(print_structure)\n",
    "\n",
    "# Explore the structure\n",
    "explore_hdf5_structure(hdf5_filename)\n",
    "\n",
    "# Load and examine the data\n",
    "print(f\"\\nLoading data for analysis...\")\n",
    "\n",
    "with h5py.File(hdf5_filename, \"r\") as f:\n",
    "    # Load the 4D dataset\n",
    "    datacube = f[\"4dstem_data\"][\"datacube\"][:]\n",
    "    \n",
    "    # Get metadata\n",
    "    scan_shape = f[\"4dstem_data\"][\"datacube\"].attrs[\"scan_shape\"]\n",
    "    detector_shape = f[\"4dstem_data\"][\"datacube\"].attrs[\"detector_shape\"]\n",
    "    \n",
    "    print(f\"âœ“ Loaded 4D datacube: {datacube.shape}\")\n",
    "    print(f\"âœ“ Scan dimensions: {scan_shape}\")\n",
    "    print(f\"âœ“ Detector dimensions: {detector_shape}\")\n",
    "    \n",
    "    # Calculate some basic statistics\n",
    "    total_counts = np.sum(datacube)\n",
    "    mean_counts = np.mean(datacube)\n",
    "    max_counts = np.max(datacube)\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"Total counts: {total_counts:,}\")\n",
    "    print(f\"Mean counts per pixel: {mean_counts:.2f}\")\n",
    "    print(f\"Maximum counts: {max_counts}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db409ca1",
   "metadata": {},
   "source": [
    "## Part 4: Basic EM Data Visualization\n",
    "\n",
    "Now we'll create meaningful visualizations of our simulated 4D-STEM data, demonstrating common analysis techniques used in electron microscopy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2644b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create comprehensive visualizations of 4D-STEM data\n",
    "\"\"\"\n",
    "\n",
    "# Set up matplotlib for high-quality figures\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"4D-STEM Data Visualization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the data\n",
    "with h5py.File(hdf5_filename, \"r\") as f:\n",
    "    datacube = f[\"4dstem_data\"][\"datacube\"][:]\n",
    "\n",
    "# 1. Virtual Bright Field (VBF) Image\n",
    "print(\"Creating Virtual Bright Field image...\")\n",
    "\n",
    "# Define central detector region (bright field)\n",
    "center_x, center_y = detector_shape[0]//2, detector_shape[1]//2\n",
    "radius = 15  # pixels\n",
    "\n",
    "# Create circular mask for bright field detector\n",
    "y, x = np.ogrid[:detector_shape[0], :detector_shape[1]]\n",
    "bf_mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
    "\n",
    "# Sum over detector pixels within bright field region\n",
    "vbf_image = np.sum(datacube * bf_mask, axis=(2, 3))\n",
    "\n",
    "# 2. Virtual Dark Field (VDF) Image\n",
    "print(\"Creating Virtual Dark Field image...\")\n",
    "\n",
    "# Define annular dark field detector (outer ring)\n",
    "inner_radius = 25\n",
    "outer_radius = 50\n",
    "\n",
    "df_mask = ((x - center_x)**2 + (y - center_y)**2 >= inner_radius**2) & \\\n",
    "          ((x - center_x)**2 + (y - center_y)**2 <= outer_radius**2)\n",
    "\n",
    "vdf_image = np.sum(datacube * df_mask, axis=(2, 3))\n",
    "\n",
    "# 3. Average Diffraction Pattern\n",
    "print(\"Computing average diffraction pattern...\")\n",
    "\n",
    "avg_dp = np.mean(datacube, axis=(0, 1))\n",
    "\n",
    "# 4. Selected Area Diffraction Patterns\n",
    "print(\"Extracting selected area diffraction patterns...\")\n",
    "\n",
    "# Select a few interesting positions\n",
    "positions = [(10, 10), (32, 32), (50, 50)]\n",
    "selected_dps = [datacube[x, y] for x, y in positions]\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Virtual Bright Field\n",
    "ax1 = plt.subplot(2, 4, 1)\n",
    "im1 = ax1.imshow(vbf_image, cmap='gray', origin='lower')\n",
    "ax1.set_title('Virtual Bright Field (VBF)')\n",
    "ax1.set_xlabel('Scan X (pixels)')\n",
    "ax1.set_ylabel('Scan Y (pixels)')\n",
    "plt.colorbar(im1, ax=ax1, label='Intensity')\n",
    "\n",
    "# Virtual Dark Field\n",
    "ax2 = plt.subplot(2, 4, 2)\n",
    "im2 = ax2.imshow(vdf_image, cmap='hot', origin='lower')\n",
    "ax2.set_title('Virtual Dark Field (VDF)')\n",
    "ax2.set_xlabel('Scan X (pixels)')\n",
    "ax2.set_ylabel('Scan Y (pixels)')\n",
    "plt.colorbar(im2, ax=ax2, label='Intensity')\n",
    "\n",
    "# Average Diffraction Pattern\n",
    "ax3 = plt.subplot(2, 4, 3)\n",
    "im3 = ax3.imshow(avg_dp, cmap='viridis', origin='lower', \n",
    "                 norm=plt.Normalize(vmin=0, vmax=np.percentile(avg_dp, 99)))\n",
    "ax3.set_title('Average Diffraction Pattern')\n",
    "ax3.set_xlabel('Detector X (pixels)')\n",
    "ax3.set_ylabel('Detector Y (pixels)')\n",
    "\n",
    "# Add circles to show virtual detector positions\n",
    "circle_bf = plt.Circle((center_x, center_y), radius, fill=False, color='red', linewidth=2)\n",
    "circle_df_inner = plt.Circle((center_x, center_y), inner_radius, fill=False, color='yellow', linewidth=2)\n",
    "circle_df_outer = plt.Circle((center_x, center_y), outer_radius, fill=False, color='yellow', linewidth=2)\n",
    "ax3.add_patch(circle_bf)\n",
    "ax3.add_patch(circle_df_inner)\n",
    "ax3.add_patch(circle_df_outer)\n",
    "\n",
    "plt.colorbar(im3, ax=ax3, label='Intensity')\n",
    "\n",
    "# Virtual detector diagram\n",
    "ax4 = plt.subplot(2, 4, 4)\n",
    "detector_diagram = np.zeros(detector_shape)\n",
    "detector_diagram[bf_mask] = 1  # Bright field\n",
    "detector_diagram[df_mask] = 2  # Dark field\n",
    "\n",
    "im4 = ax4.imshow(detector_diagram, cmap='viridis', origin='lower')\n",
    "ax4.set_title('Virtual Detector Configuration')\n",
    "ax4.set_xlabel('Detector X (pixels)')\n",
    "ax4.set_ylabel('Detector Y (pixels)')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='purple', label='Background'),\n",
    "                  Patch(facecolor='yellow', label='Bright Field'),\n",
    "                  Patch(facecolor='green', label='Dark Field')]\n",
    "ax4.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# Selected area diffraction patterns\n",
    "for i, (pos, dp) in enumerate(zip(positions, selected_dps)):\n",
    "    ax = plt.subplot(2, 4, 5 + i)\n",
    "    im = ax.imshow(dp, cmap='plasma', origin='lower',\n",
    "                   norm=plt.Normalize(vmin=0, vmax=np.percentile(dp, 99)))\n",
    "    ax.set_title(f'Diffraction Pattern\\nPosition ({pos[0]}, {pos[1]})')\n",
    "    ax.set_xlabel('Detector X (pixels)')\n",
    "    ax.set_ylabel('Detector Y (pixels)')\n",
    "    \n",
    "    if i == 0:  # Add colorbar to first DP\n",
    "        plt.colorbar(im, ax=ax, label='Intensity')\n",
    "\n",
    "# Add position markers to VBF image\n",
    "for pos in positions:\n",
    "    ax1.plot(pos[1], pos[0], 'ro', markersize=8, markerfacecolor='none', markeredgewidth=2)\n",
    "    ax1.text(pos[1]+2, pos[0]+2, f'({pos[0]},{pos[1]})', color='red', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('4dstem_analysis_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print analysis summary\n",
    "print(f\"\\n4D-STEM Analysis Summary:\")\n",
    "print(f\"Dataset shape: {datacube.shape}\")\n",
    "print(f\"VBF image shape: {vbf_image.shape}\")\n",
    "print(f\"VDF image shape: {vdf_image.shape}\")\n",
    "print(f\"Average DP shape: {avg_dp.shape}\")\n",
    "print(f\"BF detector area: {np.sum(bf_mask)} pixels\")\n",
    "print(f\"DF detector area: {np.sum(df_mask)} pixels\")\n",
    "\n",
    "# Calculate contrast metrics\n",
    "vbf_contrast = (np.max(vbf_image) - np.min(vbf_image)) / np.mean(vbf_image)\n",
    "vdf_contrast = (np.max(vdf_image) - np.min(vdf_image)) / np.mean(vdf_image)\n",
    "\n",
    "print(f\"VBF contrast: {vbf_contrast:.3f}\")\n",
    "print(f\"VDF contrast: {vdf_contrast:.3f}\")\n",
    "\n",
    "print(\"\\nâœ“ 4D-STEM visualization complete\")\n",
    "print(\"âœ“ Figure saved as '4dstem_analysis_overview.png'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fedf35d",
   "metadata": {},
   "source": [
    "## Part 5: Reproducible Research Practices\n",
    "\n",
    "Document your work with proper metadata, version information, and analysis parameters to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd4340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate analysis report with complete metadata for reproducibility\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Reproducible Research Documentation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive analysis metadata\n",
    "analysis_metadata = {\n",
    "    \"analysis_info\": {\n",
    "        \"assignment\": \"MS 465 Assignment 1: Environment Setup\",\n",
    "        \"date\": datetime.now().isoformat(),\n",
    "        \"analyst\": getpass.getuser(),  # Current username\n",
    "        \"python_version\": sys.version,\n",
    "        \"platform\": platform.platform()\n",
    "    },\n",
    "    \n",
    "    \"data_info\": {\n",
    "        \"filename\": hdf5_filename,\n",
    "        \"file_size_bytes\": os.path.getsize(hdf5_filename),\n",
    "        \"data_shape\": list(datacube.shape),\n",
    "        \"data_type\": str(datacube.dtype),\n",
    "        \"total_counts\": int(np.sum(datacube)),\n",
    "        \"data_hash\": hashlib.md5(datacube.tobytes()).hexdigest()\n",
    "    },\n",
    "    \n",
    "    \"analysis_parameters\": {\n",
    "        \"bf_detector_radius\": radius,\n",
    "        \"df_detector_inner_radius\": inner_radius,\n",
    "        \"df_detector_outer_radius\": outer_radius,\n",
    "        \"selected_positions\": positions,\n",
    "        \"visualization_percentile\": 99\n",
    "    },\n",
    "    \n",
    "    \"results\": {\n",
    "        \"vbf_contrast\": float(vbf_contrast),\n",
    "        \"vdf_contrast\": float(vdf_contrast),\n",
    "        \"bf_detector_area_pixels\": int(np.sum(bf_mask)),\n",
    "        \"df_detector_area_pixels\": int(np.sum(df_mask)),\n",
    "        \"avg_dp_mean_intensity\": float(np.mean(avg_dp)),\n",
    "        \"avg_dp_max_intensity\": float(np.max(avg_dp))\n",
    "    },\n",
    "    \n",
    "    \"software_versions\": {}\n",
    "}\n",
    "\n",
    "# Add package versions\n",
    "key_packages = ['numpy', 'matplotlib', 'h5py', 'scipy']\n",
    "for package in key_packages:\n",
    "    try:\n",
    "        module = __import__(package)\n",
    "        version = getattr(module, '__version__', 'Unknown')\n",
    "        analysis_metadata[\"software_versions\"][package] = version\n",
    "    except ImportError:\n",
    "        analysis_metadata[\"software_versions\"][package] = \"Not installed\"\n",
    "\n",
    "# Save metadata as JSON\n",
    "metadata_filename = \"analysis_metadata.json\"\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(analysis_metadata, f, indent=2)\n",
    "\n",
    "print(f\"Analysis Metadata Summary:\")\n",
    "print(f\"Analyst: {analysis_metadata['analysis_info']['analyst']}\")\n",
    "print(f\"Date: {analysis_metadata['analysis_info']['date']}\")\n",
    "print(f\"Data file: {analysis_metadata['data_info']['filename']}\")\n",
    "print(f\"Data hash: {analysis_metadata['data_info']['data_hash'][:16]}...\")\n",
    "print(f\"Analysis results saved to: {metadata_filename}\")\n",
    "\n",
    "# Create a summary report\n",
    "report = f\"\"\"\n",
    "# MS 465 Assignment 1 Analysis Report\n",
    "\n",
    "## Analysis Summary\n",
    "- **Date**: {analysis_metadata['analysis_info']['date']}\n",
    "- **Analyst**: {analysis_metadata['analysis_info']['analyst']}\n",
    "- **Dataset**: {analysis_metadata['data_info']['filename']}\n",
    "- **Data Shape**: {analysis_metadata['data_info']['data_shape']}\n",
    "\n",
    "## Key Results\n",
    "- **VBF Contrast**: {analysis_metadata['results']['vbf_contrast']:.3f}\n",
    "- **VDF Contrast**: {analysis_metadata['results']['vdf_contrast']:.3f}\n",
    "- **Total Counts**: {analysis_metadata['data_info']['total_counts']:,}\n",
    "\n",
    "## Virtual Detector Configuration\n",
    "- **Bright Field Radius**: {analysis_metadata['analysis_parameters']['bf_detector_radius']} pixels\n",
    "- **Dark Field Inner Radius**: {analysis_metadata['analysis_parameters']['df_detector_inner_radius']} pixels\n",
    "- **Dark Field Outer Radius**: {analysis_metadata['analysis_parameters']['df_detector_outer_radius']} pixels\n",
    "\n",
    "## Software Environment\n",
    "\"\"\"\n",
    "\n",
    "for package, version in analysis_metadata['software_versions'].items():\n",
    "    report += f\"- **{package}**: {version}\\n\"\n",
    "\n",
    "report += f\"\\n## Reproducibility Information\\n\"\n",
    "report += f\"- **Data Hash**: `{analysis_metadata['data_info']['data_hash']}`\\n\"\n",
    "report += f\"- **Python Version**: {sys.version.split()[0]}\\n\"\n",
    "report += f\"- **Platform**: {platform.platform()}\\n\"\n",
    "\n",
    "# Save report\n",
    "report_filename = \"analysis_report.md\"\n",
    "with open(report_filename, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nâœ“ Metadata saved to: {metadata_filename}\")\n",
    "print(f\"âœ“ Report saved to: {report_filename}\")\n",
    "\n",
    "# Display file summary\n",
    "files_created = [\n",
    "    hdf5_filename,\n",
    "    \"4dstem_analysis_overview.png\",\n",
    "    metadata_filename,\n",
    "    report_filename\n",
    "]\n",
    "\n",
    "print(f\"\\nFiles Created This Session:\")\n",
    "print(\"-\" * 40)\n",
    "for filename in files_created:\n",
    "    if os.path.exists(filename):\n",
    "        size = os.path.getsize(filename)\n",
    "        print(f\"âœ“ {filename:<30} ({size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"âœ— {filename:<30} (not found)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3e1d7",
   "metadata": {},
   "source": [
    "## Part 6: Assignment Completion Checklist\n",
    "\n",
    "Before submitting this assignment, verify that you have completed all required components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eaa429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assignment completion verification\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ASSIGNMENT 1 COMPLETION CHECKLIST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checklist_items = [\n",
    "    (\"Environment Verification\", \"All required packages installed and working\"),\n",
    "    (\"Git Configuration\", \"Git configured with user name and email\"),\n",
    "    (\"GitHub Repository\", \"Repository created and properly structured\"),\n",
    "    (\"HDF5 Data Management\", \"Sample EM data created and stored with metadata\"),\n",
    "    (\"Data Visualization\", \"4D-STEM analysis plots generated\"),\n",
    "    (\"Reproducibility Documentation\", \"Analysis metadata and report created\"),\n",
    "    (\"Code Quality\", \"Code is well-documented with comments\"),\n",
    "    (\"File Organization\", \"All files properly organized and named\")\n",
    "]\n",
    "\n",
    "print(\"Please verify the following items are complete:\")\n",
    "print()\n",
    "\n",
    "for i, (item, description) in enumerate(checklist_items, 1):\n",
    "    print(f\"{i}. â˜ {item}\")\n",
    "    print(f\"   {description}\")\n",
    "    print()\n",
    "\n",
    "print(\"SUBMISSION REQUIREMENTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. Complete this Jupyter notebook with all cells executed\")\n",
    "print(\"2. Commit all files to your GitHub repository with meaningful messages\")\n",
    "print(\"3. Include the following files in your repository:\")\n",
    "\n",
    "required_files = [\n",
    "    \"assignment_01_environment_setup.ipynb\",\n",
    "    \"sample_4dstem_data.h5\", \n",
    "    \"4dstem_analysis_overview.png\",\n",
    "    \"analysis_metadata.json\",\n",
    "    \"analysis_report.md\",\n",
    "    \"README.md (repository description)\"\n",
    "]\n",
    "\n",
    "for filename in required_files:\n",
    "    print(f\"   - {filename}\")\n",
    "\n",
    "print(\"\\n4. Submit your GitHub repository URL via Canvas\")\n",
    "print(\"5. Ensure repository is public or accessible to instructor\")\n",
    "\n",
    "print(\"\\nGRADING CRITERIA:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"- Environment Setup (25%): All packages working correctly\")\n",
    "print(\"- GitHub Integration (25%): Proper version control workflow\")\n",
    "print(\"- Code Quality (25%): Clean, documented, reproducible code\")\n",
    "print(\"- Data Analysis (25%): Successful EM data processing and visualization\")\n",
    "\n",
    "print(f\"\\nDUE DATE: Friday, January 9, 2026 at 11:59 PM\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Final system check\n",
    "print(\"FINAL SYSTEM STATUS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    # Check if all required files exist\n",
    "    files_exist = all(os.path.exists(f) for f in files_created)\n",
    "    print(f\"âœ“ All output files created: {files_exist}\")\n",
    "    \n",
    "    # Check if in git repository\n",
    "    git_status = run_git_command(\"git status --porcelain\")\n",
    "    in_git_repo = \"Error:\" not in git_status\n",
    "    print(f\"âœ“ In Git repository: {in_git_repo}\")\n",
    "    \n",
    "    # Check package availability\n",
    "    core_packages = ['numpy', 'matplotlib', 'h5py', 'scipy']\n",
    "    packages_ok = True\n",
    "    for pkg in core_packages:\n",
    "        try:\n",
    "            __import__(pkg)\n",
    "        except ImportError:\n",
    "            packages_ok = False\n",
    "            break\n",
    "    print(f\"âœ“ Core packages available: {packages_ok}\")\n",
    "    \n",
    "    overall_status = files_exist and in_git_repo and packages_ok\n",
    "    \n",
    "    if overall_status:\n",
    "        print(\"\\nStatus: âœ… READY FOR SUBMISSION\")\n",
    "        print(\"Your environment is properly configured and all components are working!\")\n",
    "    else:\n",
    "        print(\"\\nStatus: âš ï¸  NEEDS ATTENTION\")\n",
    "        print(\"Please review the checklist and fix any issues before submitting.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nStatus: âŒ ERROR\")\n",
    "    print(f\"Error during status check: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
